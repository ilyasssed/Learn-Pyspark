{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Download\n",
    "\n",
    "First of all, you need to download spark and unzip it in a folder on your computer. You can download the zip file in here: https://spark.apache.org/downloads.html <br/>\n",
    "After that you need to add 3 environment variables. For Windows users, you can follow those steps:\n",
    "- Press Windows+R and run the command: sysdm.cpl\n",
    "- Click on Advanced > Environment Variables\n",
    "- Add those variables in system variables by clicking new:\n",
    "    - SPARK_HOME: add the path where you unzipped the spark zip file\n",
    "    - HADOOP_HOME: add the same path in here\n",
    "    - JAVA_HOME: add the java directory path (if you don't have java installed, just search download java jdk on google. I am using the jdk1.8.0)\n",
    "- You should also add the bin folder of spark in your path variable (YOURPATH/bin)\n",
    "- Download the hadoop.dll and winutils.exe from here: https://github.com/cdarlint/winutils, and put them in the spark bin folder\n",
    "\n",
    "\n",
    "To make sure that your spark is working you can just run the command \"pyspark\" in cmd.<br/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import findspark to find spark and import it\n",
    "import findspark\n",
    "findspark.init()\n",
    "# Import spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a session\n",
    "spark = (SparkSession\n",
    "        .builder\n",
    "        .appName(\"PythonMnMCount\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the M&M data set file name\n",
    "mnm_file = \"data/mnm_dataset.csv\"\n",
    "# read the file into a Spark DataFrame\n",
    "mnm_df = (spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(mnm_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Count|\n",
      "+-----+------+-----+\n",
      "|TX   |Red   |20   |\n",
      "|NV   |Blue  |66   |\n",
      "|CO   |Blue  |79   |\n",
      "|OR   |Blue  |71   |\n",
      "|WA   |Yellow|93   |\n",
      "+-----+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame\n",
    "mnm_df.show(5, truncate=False) #if truncate is true, truncate strings longer than 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate count of all colors and groupBy state and color\n",
    "# orderBy descending order\n",
    "count_mnm_df = (mnm_df.select(\"State\", \"Color\", \"Count\")\n",
    "                .groupBy(\"State\", \"Color\")\n",
    "                .sum(\"Count\")\n",
    "                .orderBy(\"sum(Count)\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "|State|Color |sum(Count)|\n",
      "+-----+------+----------+\n",
      "|CA   |Yellow|100956    |\n",
      "|WA   |Green |96486     |\n",
      "|CA   |Brown |95762     |\n",
      "|TX   |Green |95753     |\n",
      "|TX   |Red   |95404     |\n",
      "|CO   |Yellow|95038     |\n",
      "|NM   |Red   |94699     |\n",
      "|OR   |Orange|94514     |\n",
      "|WY   |Green |94339     |\n",
      "|NV   |Orange|93929     |\n",
      "|TX   |Yellow|93819     |\n",
      "|CO   |Green |93724     |\n",
      "|CO   |Brown |93692     |\n",
      "|CA   |Green |93505     |\n",
      "|NM   |Brown |93447     |\n",
      "|CO   |Blue  |93412     |\n",
      "|WA   |Red   |93332     |\n",
      "|WA   |Brown |93082     |\n",
      "|WA   |Yellow|92920     |\n",
      "|NM   |Yellow|92747     |\n",
      "|NV   |Brown |92478     |\n",
      "|TX   |Orange|92315     |\n",
      "|AZ   |Brown |92287     |\n",
      "|AZ   |Green |91882     |\n",
      "|WY   |Red   |91768     |\n",
      "|AZ   |Orange|91684     |\n",
      "|CA   |Red   |91527     |\n",
      "|WA   |Orange|91521     |\n",
      "|NV   |Yellow|91390     |\n",
      "|UT   |Orange|91341     |\n",
      "|NV   |Green |91331     |\n",
      "|NM   |Orange|91251     |\n",
      "|NM   |Green |91160     |\n",
      "|WY   |Blue  |91002     |\n",
      "|UT   |Red   |90995     |\n",
      "|CO   |Orange|90971     |\n",
      "|AZ   |Yellow|90946     |\n",
      "|TX   |Brown |90736     |\n",
      "|OR   |Blue  |90526     |\n",
      "|CA   |Orange|90311     |\n",
      "|OR   |Red   |90286     |\n",
      "|NM   |Blue  |90150     |\n",
      "|AZ   |Red   |90042     |\n",
      "|NV   |Blue  |90003     |\n",
      "|UT   |Blue  |89977     |\n",
      "|AZ   |Blue  |89971     |\n",
      "|WA   |Blue  |89886     |\n",
      "|OR   |Green |89578     |\n",
      "|CO   |Red   |89465     |\n",
      "|NV   |Red   |89346     |\n",
      "|UT   |Yellow|89264     |\n",
      "|OR   |Brown |89136     |\n",
      "|CA   |Blue  |89123     |\n",
      "|UT   |Brown |88973     |\n",
      "|TX   |Blue  |88466     |\n",
      "|UT   |Green |88392     |\n",
      "|OR   |Yellow|88129     |\n",
      "|WY   |Orange|87956     |\n",
      "|WY   |Yellow|87800     |\n",
      "|WY   |Brown |86110     |\n",
      "+-----+------+----------+\n",
      "\n",
      "Total Rows = 60\n"
     ]
    }
   ],
   "source": [
    "# show all the resulting aggregation for all the dates and colors\n",
    "count_mnm_df.show(n=60, truncate=False)\n",
    "print(\"Total Rows = %d\" % (count_mnm_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the aggregate count for California by filtering\n",
    "ca_count_mnm_df = (mnm_df.select(\"*\")\n",
    "                   .where(mnm_df.State == 'CA')\n",
    "                   .groupBy(\"Color\")\n",
    "                   .sum(\"Count\")\n",
    "                   .orderBy(\"sum(Count)\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|Color |sum(Count)|\n",
      "+------+----------+\n",
      "|Yellow|100956    |\n",
      "|Brown |95762     |\n",
      "|Green |93505     |\n",
      "|Red   |91527     |\n",
      "|Orange|90311     |\n",
      "|Blue  |89123     |\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ca_count_mnm_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators and Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName('HelloWorld').setMaster('local')\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD of tuples (name, age)\n",
    "dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    " (\"TD\", 35), (\"Brooke\", 25)])\n",
    "# Use map and reduceByKey transformations with their lambda\n",
    "# expressions to aggregate and then compute average\n",
    "agesRDD = (dataRDD\n",
    " .map(lambda x: (x[0], (x[1], 1)))\n",
    " .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    " .map(lambda x: (x[0], x[1][0]/x[1][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same operation with operators\n",
    "# Create a DataFrame using SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"AuthorsAges\")\n",
    " .getOrCreate())\n",
    "# Create a DataFrame\n",
    "data_df = spark.createDataFrame([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30),\n",
    " (\"TD\", 35), (\"Brooke\", 25)],[\"name\", \"age\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Brooke| 20|\n",
      "| Denny| 31|\n",
      "| Jules| 30|\n",
      "|    TD| 35|\n",
      "|Brooke| 25|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "| Denny|    31.0|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_df = data_df.groupBy(\"name\").agg(avg(\"age\"))\n",
    "avg_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a schema programmatically for a DataFrame\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"author\", StringType(), False),\n",
    " StructField(\"title\", StringType(), False),\n",
    " StructField(\"pages\", IntegerType(), False)])\n",
    "# Same structure using DDL\n",
    "schema_DDL = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for our data using DDL\n",
    "schema = \"\"\"`Id` INT, \n",
    "`First` STRING, \n",
    "`Last` STRING, \n",
    "`Url` STRING,\n",
    "`Published` STRING, \n",
    "`Hits` INT, \n",
    "`Campaigns` ARRAY<STRING>\"\"\"\n",
    "# Create our static data\n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"LinkedIn\"]],\n",
    " [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\",\n",
    "\"LinkedIn\"]],\n",
    " [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    " [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568,\n",
    "[\"twitter\", \"FB\"]],\n",
    " [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\",\n",
    "\"twitter\", \"FB\", \"LinkedIn\"]],\n",
    " [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568,\n",
    "[\"twitter\", \"LinkedIn\"]]\n",
    " ]\n",
    "# Create a DataFrame using the schema\n",
    "blogs_df = spark.createDataFrame(data, schema)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|Id |First    |Last   |Url              |Published|Hits |Campaigns                   |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|1  |Jules    |Damji  |https://tinyurl.1|1/4/2016 |4535 |[twitter, LinkedIn]         |\n",
      "|2  |Brooke   |Wenig  |https://tinyurl.2|5/5/2018 |8908 |[twitter, LinkedIn]         |\n",
      "|3  |Denny    |Lee    |https://tinyurl.3|6/7/2019 |7659 |[web, twitter, FB, LinkedIn]|\n",
      "|4  |Tathagata|Das    |https://tinyurl.4|5/12/2018|10568|[twitter, FB]               |\n",
      "|5  |Matei    |Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB, LinkedIn]|\n",
      "|6  |Reynold  |Xin    |https://tinyurl.6|3/2/2015 |25568|[twitter, LinkedIn]         |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the schema used by Spark to build the DataFrame\n",
    "blogs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Id,IntegerType,true),StructField(First,StringType,true),StructField(Last,StringType,true),StructField(Url,StringType,true),StructField(Published,StringType,true),StructField(Hits,IntegerType,true),StructField(Campaigns,ArrayType(StringType,true),true)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To show the schema and use it to build a Dataframe using the same schema\n",
    "blogs_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns and Rows\n",
    "In Spark’s supported languages, columns are objects with public methods (represented by the\n",
    "Column type).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id', 'First', 'Last', 'Url', 'Published', 'Hits', 'Campaigns']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the blogs_df columns list\n",
    "blogs_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we're looking at the hits column\n",
    "blogs_df.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can use the expr function to calculate the Hits column multiplied by 2\n",
    "blogs_df.select(expr(\"Hits * 2\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also use the col function to calculate the value\n",
    "blogs_df.select(col(\"Hits\") * 2).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use an expression to compute big hitters for blogs\n",
    "# This adds a new column, Big Hitters, based on the conditional expression\n",
    "blogs_df.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate three columns, create a new column, and show the\n",
    "# newly created concatenated column\n",
    "blogs_df \\\n",
    ".withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\")))) \\\n",
    ".select(col(\"AuthorsId\")) \\\n",
    ".show(4) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort by Id column in descending order\n",
    "blogs_df.sort(col(\"Id\").desc()).show()\n",
    "# blogs_df.sort(blogs_df.Id.desc()).show() # Second method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n",
    " [\"twitter\", \"LinkedIn\"])\n",
    "# access using index for individual items\n",
    "blog_row[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can create DataFrames using rows\n",
    "rows = [Row(\"Matei Zaharia\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+------+--------------------+\n",
      "| Id|    First|   Last|              Url|Published|  Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+------+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016|  4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018|  8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019|  7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018| 10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014| 40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015| 25568| [twitter, LinkedIn]|\n",
      "|  7|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|255568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we can use the blog row and add it to the blogs_df\n",
    "blog_row = [Row(7, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 255568,\n",
    " [\"twitter\", \"LinkedIn\"])]\n",
    "newRow = spark.createDataFrame(blog_row, list(blogs_df.schema.fieldNames()))\n",
    "blogs_df.union(newRow).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common DataFrame Operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using DataFrameReader and DataFrameWriter\n",
    "\n",
    "Reading and writing are simple in Spark because of these high-level abstractions and\n",
    "contributions from the community to connect to a wide variety of data sources,\n",
    "including common NoSQL stores, RDBMSs, streaming engines such as Apache Kafka\n",
    "and Kinesis, and more.<br/>\n",
    "To get started, let’s read a large CSV file containing data on San Francisco Fire\n",
    "Department calls.<br/>\n",
    "We will define a schema for this file and use the DataFrameReader class and its methods to tell Spark what to do. Because this file contains 28 columns and over 4,380,660 records, it’s more efficient to define a schema than have Spark infer it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Programmatic way to define a schema\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    " StructField('UnitID', StringType(), True),\n",
    " StructField('IncidentNumber', IntegerType(), True),\n",
    " StructField('CallType', StringType(), True),\n",
    " StructField('CallDate', StringType(), True),\n",
    " StructField('WatchDate', StringType(), True),\n",
    " StructField('CallFinalDisposition', StringType(), True),\n",
    " StructField('AvailableDtTm', StringType(), True),\n",
    " StructField('Address', StringType(), True),\n",
    " StructField('City', StringType(), True),\n",
    " StructField('Zipcode', IntegerType(), True),\n",
    " StructField('Battalion', StringType(), True),\n",
    " StructField('StationArea', StringType(), True),\n",
    " StructField('Box', StringType(), True),\n",
    " StructField('OriginalPriority', StringType(), True),\n",
    " StructField('Priority', StringType(), True),\n",
    " StructField('FinalPriority', IntegerType(), True),\n",
    " StructField('ALSUnit', BooleanType(), True),\n",
    " StructField('CallTypeGroup', StringType(), True),\n",
    " StructField('NumAlarms', IntegerType(), True),\n",
    " StructField('UnitType', StringType(), True),\n",
    " StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    " StructField('FirePreventionDistrict', StringType(), True),\n",
    " StructField('SupervisorDistrict', StringType(), True),\n",
    " StructField('Neighborhood', StringType(), True),\n",
    " StructField('Location', StringType(), True),\n",
    " StructField('RowID', StringType(), True),\n",
    " StructField('Delay', FloatType(), True)])\n",
    "\n",
    "# Use the DataFrameReader interface to read a CSV file\n",
    "sf_fire_file = \"data/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.compression.codec\", \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can get an error if you need more memory to perform the operation, or if you don't have hadoop.dll in your spark bin directory\n",
    "parquet_path = \"data/fire_calls_parquet\"\n",
    "fire_df.write.format(\"parquet\").save(parquet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also save a parquet table which registers metadata with the Hive metastore\n",
    "parquet_table = \"parquet_fire_call_table\" # name of the table\n",
    "fire_df.write.format(\"parquet\").saveAsTable(parquet_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projections and filters\n",
    "A projection in relational parlance is a way to return only the\n",
    "rows matching a certain relational condition by using filters. In Spark, projections are\n",
    "done with the select() method, while filters can be expressed using the filter() or\n",
    "where() method. We can use this technique to examine specific aspects of our SF Fire\n",
    "Department data set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter on the initial DataFrame\n",
    "few_fire_df = (fire_df\n",
    " .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    " .where(col(\"CallType\") != \"Medical Incident\"))\n",
    "# Show the 5 first rows of the resulting DataFrame without truncating the long strings\n",
    "few_fire_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To count the number of distinct call types\n",
    "(fire_df.select(\"CallType\")\n",
    " .where(col(\"CallType\")\n",
    "        .isNotNull())\n",
    " .agg(countDistinct(\"CallType\")\n",
    "      .alias(\"DistinctCallTypes\")).show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "|Watercraft in Distress             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the distinct call types\n",
    "(fire_df\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .distinct()\n",
    " .show(10, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename column using withColumnRenamed\n",
    "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(new_fire_df\n",
    " .select(\"ResponseDelayedinMins\")\n",
    " .where(col(\"ResponseDelayedinMins\") > 5)\n",
    " .show(5, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------------------+\n",
      "|CallDate  |WatchDate |AvailableDtTm         |\n",
      "+----------+----------+----------------------+\n",
      "|01/11/2002|01/10/2002|01/11/2002 01:51:44 AM|\n",
      "|01/11/2002|01/10/2002|01/11/2002 03:01:18 AM|\n",
      "|01/11/2002|01/10/2002|01/11/2002 02:39:50 AM|\n",
      "|01/11/2002|01/10/2002|01/11/2002 04:16:46 AM|\n",
      "|01/11/2002|01/10/2002|01/11/2002 06:01:58 AM|\n",
      "+----------+----------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform date to timestamp\n",
    "fire_ts_df = (new_fire_df\n",
    " .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"CallDate\")\n",
    " .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"WatchDate\")\n",
    " .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    " \"MM/dd/yyyy hh:mm:ss a\"))\n",
    " .drop(\"AvailableDtTm\"))\n",
    "\n",
    "(fire_df.select(\"CallDate\", \"WatchDate\", \"AvailableDtTm\").show(5, False))\n",
    "(fire_ts_df\n",
    " .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    " .show(5, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see how many years of fire calls we've got in the dataframe\n",
    "(fire_ts_df\n",
    " .select(year('IncidentDate'))\n",
    " .distinct()\n",
    " .orderBy(year('IncidentDate'))\n",
    " .show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CallNumber',\n",
       " 'UnitID',\n",
       " 'IncidentNumber',\n",
       " 'CallType',\n",
       " 'CallDate',\n",
       " 'WatchDate',\n",
       " 'CallFinalDisposition',\n",
       " 'AvailableDtTm',\n",
       " 'Address',\n",
       " 'City',\n",
       " 'Zipcode',\n",
       " 'Battalion',\n",
       " 'StationArea',\n",
       " 'Box',\n",
       " 'OriginalPriority',\n",
       " 'Priority',\n",
       " 'FinalPriority',\n",
       " 'ALSUnit',\n",
       " 'CallTypeGroup',\n",
       " 'NumAlarms',\n",
       " 'UnitType',\n",
       " 'UnitSequenceInCallDispatch',\n",
       " 'FirePreventionDistrict',\n",
       " 'SupervisorDistrict',\n",
       " 'Neighborhood',\n",
       " 'Location',\n",
       " 'RowID',\n",
       " 'Delay']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            CallType| count|\n",
      "+--------------------+------+\n",
      "|    Medical Incident|113794|\n",
      "|      Structure Fire| 23319|\n",
      "|              Alarms| 19406|\n",
      "|   Traffic Collision|  7013|\n",
      "|Citizen Assist / ...|  2524|\n",
      "|               Other|  2166|\n",
      "|        Outside Fire|  2094|\n",
      "|        Vehicle Fire|   854|\n",
      "|Gas Leak (Natural...|   764|\n",
      "|        Water Rescue|   755|\n",
      "|Odor (Strange / U...|   490|\n",
      "|   Electrical Hazard|   482|\n",
      "|Elevator / Escala...|   453|\n",
      "|Smoke Investigati...|   391|\n",
      "|          Fuel Spill|   193|\n",
      "|              HazMat|   124|\n",
      "|Industrial Accidents|    94|\n",
      "|           Explosion|    89|\n",
      "|Train / Rail Inci...|    57|\n",
      "|  Aircraft Emergency|    36|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The count of call types\n",
    "(fire_df\n",
    ".select(\"CallType\")\n",
    " .groupBy(\"CallType\")\n",
    " .count()\n",
    ".orderBy(\"count\", ascending=False)\n",
    ".show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|Zipcode|count|\n",
      "+-------+-----+\n",
      "|  94102|21840|\n",
      "|  94103|20897|\n",
      "|  94110|14801|\n",
      "|  94109|14686|\n",
      "|  94124| 9236|\n",
      "|  94112| 8421|\n",
      "|  94115| 7812|\n",
      "|  94107| 6941|\n",
      "|  94122| 6355|\n",
      "|  94133| 6246|\n",
      "|  94117| 5804|\n",
      "|  94114| 5175|\n",
      "|  94118| 5157|\n",
      "|  94134| 5009|\n",
      "|  94121| 4555|\n",
      "|  94132| 4321|\n",
      "|  94105| 4236|\n",
      "|  94108| 4084|\n",
      "|  94116| 3933|\n",
      "|  94123| 3719|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The number of calls by zip code\n",
    "(fire_df\n",
    ".select(\"Zipcode\")\n",
    " .groupBy(\"Zipcode\")\n",
    " .count()\n",
    ".orderBy(\"count\", ascending=False)\n",
    ".show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------+------------------------+------------------------+\n",
      "|SumNumAlarms|AvgResponseDelayedinMins|MinResponseDelayedinMins|MaxResponseDelayedinMins|\n",
      "+------------+------------------------+------------------------+------------------------+\n",
      "|      176170|       3.892364154521585|             0.016666668|                 1844.55|\n",
      "+------------+------------------------+------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use of other operations\n",
    "(fire_ts_df\n",
    " .select(sum(\"NumAlarms\"), \n",
    "         avg(\"ResponseDelayedinMins\"), \n",
    "         min(\"ResponseDelayedinMins\"), \n",
    "         max(\"ResponseDelayedinMins\"))\n",
    " .withColumnRenamed(\"sum(NumAlarms)\",\"SumNumAlarms\")\n",
    " .withColumnRenamed(\"avg(ResponseDelayedinMins)\",\"AvgResponseDelayedinMins\")\n",
    " .withColumnRenamed(\"min(ResponseDelayedinMins)\",\"MinResponseDelayedinMins\")\n",
    " .withColumnRenamed(\"max(ResponseDelayedinMins)\",\"MaxResponseDelayedinMins\")\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional questions to answer about the dataframe\n",
    "\n",
    "• What were all the different types of fire calls in 2018?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CallNumber',\n",
       " 'UnitID',\n",
       " 'IncidentNumber',\n",
       " 'CallType',\n",
       " 'CallFinalDisposition',\n",
       " 'Address',\n",
       " 'City',\n",
       " 'Zipcode',\n",
       " 'Battalion',\n",
       " 'StationArea',\n",
       " 'Box',\n",
       " 'OriginalPriority',\n",
       " 'Priority',\n",
       " 'FinalPriority',\n",
       " 'ALSUnit',\n",
       " 'CallTypeGroup',\n",
       " 'NumAlarms',\n",
       " 'UnitType',\n",
       " 'UnitSequenceInCallDispatch',\n",
       " 'FirePreventionDistrict',\n",
       " 'SupervisorDistrict',\n",
       " 'Neighborhood',\n",
       " 'Location',\n",
       " 'RowID',\n",
       " 'ResponseDelayedinMins',\n",
       " 'IncidentDate',\n",
       " 'OnWatchDate',\n",
       " 'AvailableDtTS']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fire_ts_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-----+\n",
      "|CallType                       |count|\n",
      "+-------------------------------+-----+\n",
      "|Medical Incident               |7004 |\n",
      "|Alarms                         |1144 |\n",
      "|Structure Fire                 |906  |\n",
      "|Traffic Collision              |433  |\n",
      "|Outside Fire                   |153  |\n",
      "|Other                          |114  |\n",
      "|Citizen Assist / Service Call  |113  |\n",
      "|Gas Leak (Natural and LP Gases)|69   |\n",
      "|Water Rescue                   |43   |\n",
      "|Elevator / Escalator Rescue    |36   |\n",
      "|Electrical Hazard              |30   |\n",
      "|Vehicle Fire                   |28   |\n",
      "|Smoke Investigation (Outside)  |28   |\n",
      "|Odor (Strange / Unknown)       |10   |\n",
      "|Fuel Spill                     |10   |\n",
      "|Train / Rail Incident          |5    |\n",
      "|HazMat                         |5    |\n",
      "|Suspicious Package             |3    |\n",
      "|Explosion                      |1    |\n",
      "|Assist Police                  |1    |\n",
      "+-------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    " .where(year(\"IncidentDate\") == 2018)\n",
    ".groupBy(\"CallType\")\n",
    " .count()\n",
    " .orderBy(\"count\", ascending = False)\n",
    ".show(truncate=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• What months within the year 2018 saw the highest number of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|month(IncidentDate)|count|\n",
      "+-------------------+-----+\n",
      "|                 10| 1068|\n",
      "|                  5| 1047|\n",
      "|                  3| 1029|\n",
      "|                  8| 1021|\n",
      "|                  1| 1007|\n",
      "|                  7|  974|\n",
      "|                  6|  974|\n",
      "|                  9|  951|\n",
      "|                  4|  947|\n",
      "|                  2|  919|\n",
      "|                 11|  199|\n",
      "+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    ".where(year(\"IncidentDate\") == 2018)\n",
    ".groupBy(month(\"IncidentDate\"))\n",
    ".count()\n",
    " .orderBy(\"count\", ascending=False)\n",
    ".show())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Which neighborhood in San Francisco generated the most fire calls in 2018?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|        Neighborhood|count|\n",
      "+--------------------+-----+\n",
      "|          Tenderloin| 1393|\n",
      "|     South of Market| 1053|\n",
      "|             Mission|  913|\n",
      "|Financial Distric...|  772|\n",
      "|Bayview Hunters P...|  522|\n",
      "|    Western Addition|  352|\n",
      "|     Sunset/Parkside|  346|\n",
      "|            Nob Hill|  295|\n",
      "|        Hayes Valley|  291|\n",
      "|      Outer Richmond|  262|\n",
      "| Castro/Upper Market|  251|\n",
      "|         North Beach|  231|\n",
      "|           Excelsior|  212|\n",
      "|        Potrero Hill|  210|\n",
      "|  West of Twin Peaks|  210|\n",
      "|     Pacific Heights|  191|\n",
      "|              Marina|  191|\n",
      "|           Chinatown|  191|\n",
      "|         Mission Bay|  178|\n",
      "|      Bernal Heights|  170|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    " .select(\"Neighborhood\")\n",
    ".where(year(\"IncidentDate\") == 2018)\n",
    ".groupBy(\"Neighborhood\")\n",
    ".count()\n",
    ".orderBy(\"count\", ascending=False)\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Which neighborhoods had the worst response times to fire calls in 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+\n",
      "|        Neighborhood|sum(ResponseDelayedinMins)|\n",
      "+--------------------+--------------------------+\n",
      "|          Tenderloin|         5713.416682377458|\n",
      "|     South of Market|         4019.916673846543|\n",
      "|Financial Distric...|        3353.6333242356777|\n",
      "|             Mission|        3150.3333284556866|\n",
      "|Bayview Hunters P...|        2411.9333442747593|\n",
      "|     Sunset/Parkside|        1240.1333360522985|\n",
      "|           Chinatown|        1182.3499933183193|\n",
      "|    Western Addition|        1156.0833313167095|\n",
      "|            Nob Hill|        1120.9999947845936|\n",
      "|        Hayes Valley|         980.7833325713873|\n",
      "|      Outer Richmond|         955.7999980300665|\n",
      "| Castro/Upper Market|           954.11666418612|\n",
      "|         North Beach|         898.4166664481163|\n",
      "|  West of Twin Peaks|         880.1000022888184|\n",
      "|        Potrero Hill|         880.0166715979576|\n",
      "|           Excelsior|          834.516668587923|\n",
      "|     Pacific Heights|         798.4666626155376|\n",
      "|         Mission Bay|         686.1666701734066|\n",
      "|        Inner Sunset|          683.466663569212|\n",
      "|              Marina|         654.5166680216789|\n",
      "+--------------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    " .select(\"Neighborhood\",\"ResponseDelayedinMins\")\n",
    ".where(year(\"IncidentDate\") == 2018)\n",
    ".groupBy(\"Neighborhood\")\n",
    ".sum()\n",
    ".orderBy(\"sum(ResponseDelayedinMins)\", ascending=False)\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Which week in the year in 2018 had the most fire calls?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----+\n",
      "|weekofyear(IncidentDate)|count|\n",
      "+------------------------+-----+\n",
      "|                      22|  259|\n",
      "|                      40|  255|\n",
      "|                      43|  250|\n",
      "|                      25|  249|\n",
      "|                       1|  246|\n",
      "|                      44|  244|\n",
      "|                      32|  243|\n",
      "|                      13|  243|\n",
      "|                      11|  240|\n",
      "|                       5|  236|\n",
      "|                      18|  236|\n",
      "|                      23|  235|\n",
      "|                       2|  234|\n",
      "|                      31|  234|\n",
      "|                      42|  234|\n",
      "|                      19|  233|\n",
      "|                      10|  232|\n",
      "|                       8|  232|\n",
      "|                      34|  232|\n",
      "|                      21|  231|\n",
      "+------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(fire_ts_df\n",
    ".where(year(\"IncidentDate\") == 2018)\n",
    ".groupBy(weekofyear(\"IncidentDate\"))\n",
    ".count()\n",
    " .orderBy(\"count\", ascending=False)\n",
    ".show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Is there a correlation between neighborhood, zip code, and number of fire calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between number of fire calls and zip code is : 0.21485589193343801\n",
      "The correlation between number of fire calls and neighborhood is : -0.776636689751461\n",
      "The correlation between neighborhood and zip code is : -0.08293454442147614\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Correlation between zip code and number of fire calls\n",
    "corr_zip_code_num_fire_calls = fire_df.select(\"ZipCode\").groupBy(\"ZipCode\").count().corr(\"Zipcode\", \"count\")\n",
    "\n",
    "# Create a numerical column fro the neighborhood \n",
    "indexer = StringIndexer(inputCol=\"Neighborhood\", outputCol=\"NeighborhoodIndex\")\n",
    "corr_zip_code_neighborhood = fire_df.select(\"ZipCode\",\"Neighborhood\")\n",
    "indexed = indexer.fit(corr_zip_code_neighborhood).transform(corr_zip_code_neighborhood)\n",
    "\n",
    "# Correlation between number of fire calls and neighborhood\n",
    "corr_neighborhood_number_of_calls = indexed.groupBy('NeighborhoodIndex').count().corr(\"count\", \"NeighborhoodIndex\")\n",
    "\n",
    "print(f\"The correlation between number of fire calls and zip code is : {corr_zip_code_num_fire_calls}\")\n",
    "print(f\"The correlation between number of fire calls and neighborhood is : {corr_neighborhood_number_of_calls}\")\n",
    "print(f\"The correlation between neighborhood and zip code is : {indexed.corr('NeighborhoodIndex','ZipCode')}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Spark SQL in Spark Applications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch a Spark session\n",
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"SparkSQLExampleApp\")\n",
    ".master(\"local\")\n",
    ".enableHiveSupport()\n",
    ".config(\"spark.sql.catalogImplementation\",\"hive\")\n",
    " .getOrCreate())\n",
    "\n",
    "# Path to file\n",
    "csv_file = \"data/departuredelays.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you\n",
    "# may want to specify the schema)\n",
    "df = (spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .load(csv_file))\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "|1080600|    0|     369|   ABE|        DTW|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select all flights that have a distance higher than 1000\n",
    "\n",
    "spark.sql(\"\"\"SELECT distance, origin, destination\n",
    "FROM us_delay_flights_tbl WHERE distance > 1000\n",
    "ORDER BY distance DESC\"\"\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|1011410|  124|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1021410|  190|   SFO|        ORD|\n",
      "|1101410|  184|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|1241110|  139|   SFO|        ORD|\n",
      "|1301800|  167|   SFO|        ORD|\n",
      "|1011237|  122|   SFO|        ORD|\n",
      "|1032258|  163|   SFO|        ORD|\n",
      "|1031920|  193|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we’ll find all flights between San Francisco (SFO) and Chicago \n",
    "# (ORD) with at least a two-hour delay\n",
    "\n",
    "spark.sql(\"\"\"SELECT date, delay, origin, destination\n",
    "FROM us_delay_flights_tbl WHERE origin = 'SFO' AND destination = 'ORD' AND delay >= 120\n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A more complicated request using CASE clause in SQL to treat different cases \n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    " CASE\n",
    " WHEN delay > 360 THEN 'Very Long Delays'\n",
    " WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    " WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    " WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    " WHEN delay = 0 THEN 'No Delays'\n",
    " ELSE 'Early'\n",
    " END AS Flight_Delays\n",
    " FROM us_delay_flights_tbl\n",
    " ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a managed table\n",
    "\n",
    "# Create a database called learn_spark_db\n",
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "# Tell Spark that we want to use this dataframe\n",
    "spark.sql(\"USE learn_spark_db\")\n",
    "\n",
    "# Create a table\n",
    "spark.sql(\n",
    "    \"\"\"CREATE TABLE managed_us_delay_flights_tbl \n",
    "    (date STRING, delay INT,distance INT, origin STRING, destination STRING)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------+------+-----------+\n",
      "|date|delay|distance|origin|destination|\n",
      "+----+-----+--------+------+-----------+\n",
      "+----+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"FROM managed_us_delay_flights_tbl SELECT *\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an unmanaged table\n",
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,\n",
    " distance INT, origin STRING, destination STRING)\n",
    " USING csv OPTIONS (PATH\n",
    " '/data/departuredelays.csv')\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+--------+------+-----------+\n",
      "|date|delay|distance|origin|destination|\n",
      "+----+-----+--------+------+-----------+\n",
      "+----+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"FROM learn_spark_db.us_delay_flights_tbl SELECT *\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|1011250|   55|   SFO|        JFK|\n",
      "|1012230|    0|   SFO|        JFK|\n",
      "|1010705|   -7|   SFO|        JFK|\n",
      "|1010620|   -3|   SFO|        MIA|\n",
      "|1010915|   -3|   SFO|        LAX|\n",
      "|1011005|   -8|   SFO|        DFW|\n",
      "|1011800|    0|   SFO|        ORD|\n",
      "|1011740|   -7|   SFO|        LAX|\n",
      "|1012015|   -7|   SFO|        LAX|\n",
      "|1012110|   -1|   SFO|        MIA|\n",
      "|1011610|  134|   SFO|        DFW|\n",
      "|1011240|   -6|   SFO|        MIA|\n",
      "|1010755|   -3|   SFO|        DFW|\n",
      "|1010020|    0|   SFO|        DFW|\n",
      "|1010705|   -6|   SFO|        LAX|\n",
      "|1010925|   -3|   SFO|        ORD|\n",
      "|1010555|   -6|   SFO|        ORD|\n",
      "|1011105|   -8|   SFO|        DFW|\n",
      "|1012330|   32|   SFO|        ORD|\n",
      "|1011330|    3|   SFO|        DFW|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|1010900|   14|   JFK|        LAX|\n",
      "|1011200|   -3|   JFK|        LAX|\n",
      "|1011900|    2|   JFK|        LAX|\n",
      "|1011700|   11|   JFK|        LAS|\n",
      "|1010800|   -1|   JFK|        SFO|\n",
      "|1011540|   -4|   JFK|        DFW|\n",
      "|1011705|    5|   JFK|        SAN|\n",
      "|1011530|   -3|   JFK|        SFO|\n",
      "|1011630|   -3|   JFK|        SJU|\n",
      "|1011345|    2|   JFK|        LAX|\n",
      "|1011545|   -3|   JFK|        LAX|\n",
      "|1011510|   -1|   JFK|        MIA|\n",
      "|1011745|    7|   JFK|        SFO|\n",
      "|1011250|    3|   JFK|        BOS|\n",
      "|1011645|  142|   JFK|        LAX|\n",
      "|1012135|   -2|   JFK|        LAX|\n",
      "|1011715|   18|   JFK|        ORD|\n",
      "|1011615|   25|   JFK|        IAH|\n",
      "|1011850|   -2|   JFK|        SEA|\n",
      "|1011725|   -5|   JFK|        BOS|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create views\n",
    "\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")\n",
    "\n",
    "# Views are temporary, they disappear when you end the spark session\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    " SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE\n",
    " origin = 'SFO'\n",
    " \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'JFK'\n",
    "\"\"\")\n",
    "\n",
    "# We use the prefix global_temp.<view_name> because Spark creates\n",
    "# global temporary views in a global temporary databse called global_temp\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view\"\"\").show()\n",
    "\n",
    "#The normal temporary view can be accessed directly\n",
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM us_origin_airport_JFK_tmp_view\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/C:/Users/huawie/Learn%20Pyspark/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/C:/Users/huawie/Learn%20Pyspark/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access the metadata\n",
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")\n",
    "spark.catalog.listTables()\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(1, 9).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|Cubed|\n",
      "+---+-----+\n",
      "|  1|    1|\n",
      "|  2|    8|\n",
      "|  3|   27|\n",
      "|  4|   64|\n",
      "|  5|  125|\n",
      "|  6|  216|\n",
      "|  7|  343|\n",
      "|  8|  512|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# UDF (user defined functions)\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Create cubed function\n",
    "def cubed(s):\n",
    "    return s * s * s\n",
    "\n",
    "# Register UDF\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "\n",
    "# Generate temporary view\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")\n",
    "\n",
    "# Use the UDF on the temporary view\n",
    "spark.sql(\"\"\"\n",
    "FROM udf_test\n",
    "SELECT id, cubed(id) AS Cubed\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
